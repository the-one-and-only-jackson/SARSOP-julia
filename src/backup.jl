function backup_all(tree, Î“, BN)
    for AN in children(BN)
        for BNâ€² in children(AN)
            backup_all(tree, Î“, BNâ€²)
        end
    end
    Backup(tree, Î“, BN)
    nothing
end


# ========== Algorithm 2 ==========
# Perform Î±-vector backup at a node b of T_R
function Backup(tree::BeliefTree, Î“::AlphaVectorPolicy, parent::BeliefNode)
    # 1. Î±_{a,o} â† argmax_Î± (Î± â‹… Ï„(b,a,o)), âˆ€ aâˆˆð’œ, oâˆˆð’ª
    # 2. Î±_a(s) â† R(s,a) + Î³ âˆ‘_{o,sâ€²} T(s,a,sâ€²)Z(sâ€²,a,o)Î±_{a,o}(sâ€²), âˆ€ aâˆˆð’œ, sâˆˆð’®
    # 3. Î±â€² â† argmax(Î±_a â‹… b, for a in ð’œ)
    # 4. Insert Î±â€² into Î“.

    # https://www.overleaf.com/read/rwfcytcbvrtz
    # lightweight calcultion of optimal action, then more intensive calcultion
    # of the alpha vector correspodning to that belief/action

    pomdp, Î±_vectors, action_map = Î“.pomdp, Î“.alphas, Î“.action_map

    a_opt = rand(POMDPs.actiontype(pomdp))
    V = -Inf

    for AN in children(parent)
        _sum = 0.0

        for BNâ€² in children(AN)
            b = belief(BNâ€²)
            _sum += norm_const(BNâ€²) * maximum(Î± â‹… b for Î± in Î±_vectors)
        end

        QÌ² = reward(AN) + discount(pomdp)*_sum

        if QÌ² > V
            V = QÌ²
            a_opt = value(AN)
        end
    end

    set_LB(parent, V)

    push!(action_map, a_opt)
    push!(Î±_vectors, calc_Î±(Î“, parent, a_opt))
    

    return AlphaVectorPolicy(pomdp, Î±_vectors, action_map)
end


function calc_Î±(Î“::AlphaVectorPolicy, parent::BeliefNode, a)
    belief = value(parent)
    pomdp, ð’®, b = belief.pomdp, belief.state_list, belief.b

    Î±â€² = similar(b)

    AN = get_ActionNode!(parent, a)

    for (si,s) in enumerate(ð’®) # this may need to be refined in the future
        _sum = 0.0
        for (sâ€²,T) in weighted_iterator(transition(pomdp,s,a))
            Z = POMDPs.observation(pomdp,a,sâ€²)
            for BN in children(AN)
                o = observation(BN)
                bâ€² = BN.value.b
                Î±_ao = argmax(Î±->Î±â‹…bâ€², Î“.alphas)
                _sum += T * pdf(Z,o) * Î±_ao[POMDPs.stateindex(pomdp,sâ€²)]
            end
        end

        Î±â€²[si] = POMDPs.reward(pomdp,s,a) + POMDPs.discount(pomdp) * _sum
    end

    return Î±â€²
end