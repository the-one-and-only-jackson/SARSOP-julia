# ========== Algorithm 1 ==========
function solve(solver::SARSOPSolver, pomdp::POMDP)
    # 1. Initialize the set Î“ of Î±-vectors, representing the lower bound VÌ² on the 
    #       optimal value function Vâˆ—. Initialize the upper bound VÌ„ on Vâˆ—.
    # 2. Insert the initial belief point b0 as the root of the tree T_R.
    # 3. repeat 
    # 4. SAMPLE(T_R, Î“)
    # 5.    Choose a subset of nodes from T_R. For each chosen node b, BACKUP(T_R,Î“,b).
    # 6.    PRUNE(T_R, Î“)
    # 7. until termination conditions are satisfied
    # 8. return Î“

    Î“ = alpha_init(pomdp)
    tree = BeliefTree(pomdp)

    start_time = time_ns()
    while time_ns()-start_time < solver.max_time
        Sample(solver, tree, Î“)
        backup_all(tree, Î“, tree.root)
        PRUNE(tree, Î“)
    end

    return Î“ # return alpha vectors and corresponding actions
end

function alpha_init(pomdp)
    S = ordered_states(pomdp)
    A = ordered_actions(pomdp)
    Î³ = discount(pomdp)
    r = StateActionReward(pomdp)
    
    Î±_init = 1 / (1 - Î³) * maximum(minimum(r(s, a) for s in S) for a in A)
    Î“ = [fill(Î±_init, length(S)) for a in A]

    return AlphaVectorPolicy(pomdp, Î“, A)
end

function backup_all(tree, Î“, BN)
    for AN in children(BN)
        for BNâ€² in children(AN)
            backup_all(tree, Î“, BNâ€²)
        end
    end
    Backup(tree, Î“, BN)
    nothing
end


# ========== Algorithm 2 ==========
# Perform Î±-vector backup at a node b of T_R
function Backup(tree::BeliefTree, Î“::AlphaVectorPolicy, parent::BeliefNode)
    # 1. Î±_{a,o} â† argmax_Î± (Î± â‹… Ï„(b,a,o)), âˆ€ aâˆˆğ’œ, oâˆˆğ’ª
    # 2. Î±_a(s) â† R(s,a) + Î³ âˆ‘_{o,sâ€²} T(s,a,sâ€²)Z(sâ€²,a,o)Î±_{a,o}(sâ€²), âˆ€ aâˆˆğ’œ, sâˆˆğ’®
    # 3. Î±â€² â† argmax(Î±_a â‹… b, for a in ğ’œ)
    # 4. Insert Î±â€² into Î“.

    # https://www.overleaf.com/read/rwfcytcbvrtz
    # lightweight calcultion of optimal action, then more intensive calcultion
    # of the alpha vector correspodning to that belief/action

    pomdp, Î±_vectors, action_map = Î“.pomdp, Î“.alphas, Î“.action_map

    a_opt = rand(POMDPs.actiontype(pomdp))
    V = -Inf

    for AN in children(parent)
        _sum = 0.0

        for BNâ€² in children(AN)
            b = belief(BNâ€²)
            _sum += norm_const(BNâ€²) * maximum(Î± â‹… b for Î± in Î±_vectors)
        end

        QÌ² = reward(AN) + discount(pomdp)*_sum

        if QÌ² > V
            V = QÌ²
            a_opt = value(AN)
        end
    end

    set_LB(parent, V)

    push!(action_map, a_opt)
    push!(Î±_vectors, calc_Î±(Î“, parent, a_opt))
    

    return AlphaVectorPolicy(pomdp, Î±_vectors, action_map)
end


function calc_Î±(Î“::AlphaVectorPolicy, parent::BeliefNode, a)
    belief = value(parent)
    pomdp, ğ’®, b = belief.pomdp, belief.state_list, belief.b

    Î±â€² = similar(b)

    AN = insert_ActionNode!(parent, a)

    for (si,s) in enumerate(ğ’®) # this may need to be refined in the future
        _sum = 0.0
        for (sâ€²,T) in weighted_iterator(transition(pomdp,s,a))
            Z = POMDPs.observation(pomdp,a,sâ€²)
            for BN in children(AN)
                o = observation(BN)
                bâ€² = belief(BN)
                _sum += T * pdf(Z,o) * argmax(Î±->Î±â‹…bâ€², Î“.alphas)
            end
        end

        Î±â€²[si] = POMDPs.reward(pomdp,s,a) + POMDPs.discount(pomdp) * _sum
    end

    return Î±â€²
end


# ========== Algorithm 3 ==========
# Sampling near R*
function Sample(solver::SARSOPSolver, tree::BeliefTree, Î“::AlphaVectorPolicy)
    # 1. Set L to the current lower bound on the value function at the root b_0 of T_R. 
    #    Set U to L + Ïµ, where Ïµ is the current target gap size at b0.
    # 2. SAMPLEPOINTS(T_R, Î“, b_0, L, U, Ïµ, 1).

    Ïµ = solver.Ïµ
    L = maximum(Î± â‹… belief(tree.root) for Î± in Î“.alphas)
    U = L + Ïµ 
    
    SamplePoints(tree, Î“, tree.root, L, U, Ïµ, 1)
end

function SamplePoints(tree::BeliefTree, Î“::AlphaVectorPolicy, BN::BeliefNode, L, U, Ïµ, t)
    # 3. Let VÌ‚ be the predicted value of V*(b).
    # 4. if VÌ‚ â‰¤ L and VÌ„ â‰¤ max{U, VÌ²(b) + ÏµÎ³^{-t}} then
    # 5.    return
    # 6. else
    # 7.    QÌ² â† max_a QÌ²(b,a)
    # 8.    Lâ€² â† max{L, QÌ²}
    # 9.    Uâ€² â† max{U, QÌ² + Î³^{-t} Ïµ}
    # 10.   aâ€² â† argmax_a QÌ„(b,a)
    # 11.   oâ€² â† argmax_o p(o|b,aâ€²) (VÌ„(Ï„(b,aâ€²,o)) - VÌ²(Ï„(b,aâ€²,o)))
    # 12.   Calculate L_t so that 
    #           Lâ€² = âˆ‘_s R(s,aâ€²)b(s) + Î³ ( p(oâ€²|b,aâ€²)L_t + âˆ‘_{oâ‰ oâ€²} p(o|b,aâ€²)VÌ²(Ï„(b,aâ€²,o)) )
    # 13.   Calculate U_t so that 
    #           Uâ€² = âˆ‘_s R(s,aâ€²)b(s) + Î³ ( p(oâ€²|b,aâ€²)U_t + âˆ‘_{oâ‰ oâ€²} p(o|b,aâ€²)VÌ²(Ï„(b,aâ€²,o)) )
    # 14.   bâ€² â† Ï„(b,aâ€²,oâ€²)
    # 15.   Insert bâ€² into T_R as a child of b.
    # 16.   SamplePoints(T_r, Î“, bâ€², L_t, U_t, Ïµ, t+1)

    (VÌ„, aâ€²) = get_UB(tree, BN)
    VÌ² = maximum(Î±->Î±â‹…belief(BN), Î“.alphas)

    pomdp = Î“.pomdp
    Î³ = discount(pomdp)

    if VÌ„ â‰¤ max(U, VÌ² + Ïµ*Î³^(-t))
        return
    end

    Lâ€² = max(L, VÌ²)
    Uâ€² = max(U, VÌ² + Î³^(-t)*Ïµ)

    bâ€² = similar(belief(BN))
    oâ€² = similar(observation(BN))
    K = 0.0 # p(oâ€²|b,aâ€²)
    max_val = -Inf
    Î“_upper = tree.qmdp_policy
    L_sum = 0.0
    U_sum = 0.0
    VÌ„ = 0.0
    VÌ² = 0.0

    for o in POMDPs.observations(pomdp)
        (bâ€²_temp, K_temp) = Ï„(BN, aâ€², o)

        temp_VÌ² = argmax(Î±->Î±â‹…bâ€²_temp, Î“.alphas)
        temp_VÌ„ = argmax(Î±->Î±â‹…bâ€²_temp, Î“_upper.alphas)

        L_sum += K_temp * temp_VÌ²
        U_sum += K_temp * temp_VÌ„

        if K_temp*(temp_VÌ„-temp_VÌ²) > max_val
            bâ€² = bâ€²_temp
            oâ€² = o
            K = K_temp
            VÌ² = temp_VÌ²
            VÌ„ = temp_VÌ„
        end
    end

    L_sum -= K * VÌ²
    U_sum -= K * VÌ„

    ğ’® = value(BN).state_list
    R = sum(b_s*POMDPs.reward(pomdp,s,aâ€²) for (s,b_s) in zip(ğ’®, b))

    L_t = ((Lâ€²-R)/Î³ - L_sum)/K
    U_t = ((Uâ€²-R)/Î³ - U_sum)/K

    insert_BeliefNode!(tree, BN, bâ€², aâ€², o, K)

    SamplePoints(tree, Î“, bâ€², L_t, U_t, Ïµ, t+1)

    return
end

# ========== needed functions ==========

function PRUNE(tree::BeliefTree, Î“::AlphaVectorPolicy) # not sure what parameters needed
    # see section III.D

    # insert code to determine which belief-action pair to prune?

    # BN = BeliefNode
    # a = action (not ActionNode)

    prune_actions(tree, tree.root, Î“)

    nothing
end

function prune_actions(tree, BN, Î“)
    b = belief(BN)
    ğ’œ = unique(tree.qmdp_policy.action_map)

    QÌ„ = [Q(tree.qmdp_policy, b, a) for a in ğ’œ]

    for AN in children(BN)
        a = value(AN)
        QÌ² = Q(Î“, b, a)
        for a in ğ’œ[QÌ² .> QÌ„]
            prune_tree(tree, BN, a)
        end
    end

    for AN in children(BN)
        for BNâ€² in children(AN)
            prune_actions(tree, BNâ€², Î“)
        end
    end

    nothing
end


# ========== helper functions ==========

function Q(Î“::AlphaVectorPolicy,b,a)
    Î±_vectors, action_map = Î“.alphas, Î“.action_map
    ğ’œ = unique(action_map)
    idx = ğ’œ .== a
    maximum(Î±->Î±â‹…b, Î±_vectors[idx])
end
Q(Î“::AlphaVectorPolicy,b::BeliefNode,a) = Q(Î“,belief(b),a)

get_UB(tree::BeliefTree, BN::BeliefNode) = get_UB(tree, belief(BN))
function get_UB(tree::BeliefTree, b)
    Î“ = tree.qmdp_policy.alphas
    ğ’œ = tree.qmdp_policy.action_map

    (V, a_idx) = findmax(Î± â‹… b for Î± in Î“)
    a = ğ’œ[a_idx]

    return (V,a)
end


function Ï„(BN::BeliefNode, a, o)
    belief = value(BN)
    pomdp, ğ’®, b = belief.pomdp, belief.state_list, belief.b

    # calculate the new belief
    bâ€² = similar(b)
    for (siâ€²,sâ€²) in enumerate(ğ’®)
        _sum = 0.0
        for (s,b_s) in zip(ğ’®,b)
            _sum += pdf(transition(pomdp,s,a),sâ€²) * b_s
        end
        bâ€²[siâ€²] = pdf(POMDPs.observation(pomdp,a,sâ€²),o) * _sum
        K += bâ€²[siâ€²]
    end
    bâ€² /= K
    
    return (bâ€², K)
end