# ========== Algorithm 1 ==========
function SARSOP_main(solver::SARSOPSolver, pomdp::POMDP)
    # 1. Initialize the set Œì of Œ±-vectors, representing the lower bound VÃ≤ on the optimal value function V‚àó. Initialize the upper bound VÃÑ on V‚àó.
    # 2. Insert the initial belief point b0 as the root of the tree T_R.
    # 3. repeat 
    # 4. SAMPLE(T_R, Œì)
    # 5.    Choose a subset of nodes from T_R. For each chosen node b, BACKUP(T_R,Œì,b).
    # 6.    PRUNE(T_R, Œì)
    # 7. until termination conditions are satisfied
    # 8. return Œì


    # how to initialize lower bound alpha vectors?
    # Œ±_vectors = ?????
    # action_map = ?????
    Œì = AlphaVectorPolicy(pomdp, Œ±_vectors, action_map)

    # how to initialize upper bound VÃÑ?
    # possibly with QMDP? the sarsop paper seems to mention this, 
    # but im not sure how this is a guaranteed upper bound
    # another issue: where do we store this? in the tree perhaps?
    # each node will need an upper bound, but that must be assigned later
    # VÃÑ = 

    
    b0 = ones(length(states(pomdp)))/length(states(pomdp)) # uniform weighted, probably wrong
    tree = Tree(b0)

    terminal_condition = false
    while !terminal_condition
        Sample(tree, Œì)

        subset_tree = [] # build this somehow
        for b in subset_tree
            Backup(tree, Œì, b)
        end

        # possibly: 
        # for ii in rand(1:tree.n_nodes)
        #     b = selectNode(tree, ii) # this function would need to be made, not difficult
        #     Backup(tree, Œì, b)
        # end

        PRUNE(tree, Œì)
    end

    return Œì # return alpha vectors and corresponding actions
end

# ========== Algorithm 2 ==========
# Perform Œ±-vector backup at a node b of T_R
function Backup(T_R::BeliefTree, Œì::AlphaVectorPolicy, parent::BeliefNode)
    # 1. Œ±_{a,o} ‚Üê argmax_Œ± (Œ± ‚ãÖ œÑ(b,a,o)), ‚àÄ a‚ààùíú, o‚ààùí™
    # 2. Œ±_a(s) ‚Üê R(s,a) + Œ≥ ‚àë_{o,s‚Ä≤} T(s,a,s‚Ä≤)Z(s‚Ä≤,a,o)Œ±_{a,o}(s‚Ä≤), ‚àÄ a‚ààùíú, s‚ààùíÆ
    # 3. Œ±‚Ä≤ ‚Üê argmax(Œ±_a ‚ãÖ b, for a in ùíú)
    # 4. Insert Œ±‚Ä≤ into Œì.

    # https://www.overleaf.com/read/rwfcytcbvrtz
    # lightweight calcultion of optimal action, then more intensive calcultion
    # of the alpha vector correspodning to that belief/action

    pomdp, Œ±_vectors, action_map = Œì.pomdp, Œì.Œ±_vectors, Œì.action_map

    a_opt = rand(POMDPs.actiontype(pomdp))
    V = -Inf

    for AN in children(parent)
        _sum = 0.0

        for BN‚Ä≤ in children(AN)
            b = belief(BN‚Ä≤)
            _sum += norm_const(BN‚Ä≤) * maximum(Œ± ‚ãÖ b for Œ± in Œì)
        end

        _sum = reward(AN) + discount(pomdp)*_sum

        if _sum > V
            V = _sum
            a_opt = value(AN)
        end
    end

    push!(action_map, a_opt)
    push!(Œ±_vectors, calc_Œ±(Œì, parent, a_opt))
    

    return AlphaVectorPolicy(pomdp, Œ±_vectors, action_map)
end

function calc_Œ±(Œì::AlphaVectorPolicy, parent::BeliefNode, a)
    belief = value(parent)
    pomdp, ùíÆ, b = belief.pomdp, belief.state_list, belief.b

    Œ±‚Ä≤ = similar(b)

    AN = insert_ActionNode!(parent, a)

    for (si,s) in enumerate(ùíÆ) # this may need to be refined in the future
        _sum = 0.0
        for (s‚Ä≤,T) in weighted_iterator(transition(pomdp,s,a))
            Z = POMDPs.observation(pomdp,a,s‚Ä≤)
            for BN in children(AN)
                o = observation(BN)
                b‚Ä≤ = belief(BN)
                _sum += T * pdf(Z,o) * argmax_(Œ±->Œ±‚ãÖb‚Ä≤, Œì)
            end
        end

        Œ±‚Ä≤[si] = POMDPs.reward(pomdp,s,a) + POMDPs.discount(pomdp) * _sum
    end

    return Œ±‚Ä≤
end

# ========== Algorithm 3 ==========
# Sampling near R*
function Sample(T_R::BeliefTree, Œì::AlphaVectorPolicy)
    # 1. Set L to the current lower bound on the value function at the root b_0 of T_R. Set U to L + œµ, where œµ is the current target gap size at b0.
    # 2. SAMPLEPOINTS(T_R, Œì, b_0, L, U, œµ, 1).

    b = T_R.b0.b
    œµ = 0 # ??????????????????? wtf is this value supposed to be, definitely not zero
    L = maximum(Œ± ‚ãÖ T_R.b0 for Œ± in Œì)
    U = L + œµ 
    
    SAMPLEPOINTS(T_R, Œì, b, L, U, œµ, 1)
end

function SamplePoints(T_R::BeliefTree, Œì::AlphaVectorPolicy, b, L, U, œµ, t)
    # 3. Let VÃÇ be the predicted value of V*(b).
    # 4. if VÃÇ ‚â§ L and VÃÑ ‚â§ max{U, VÃ≤(b) + œµŒ≥^{-t}} then
    # 5.    return
    # 6. else
    # 7.    QÃ≤ ‚Üê max_a QÃ≤(b,a)
    # 8.    L‚Ä≤ ‚Üê max{L, QÃ≤}
    # 9.    U‚Ä≤ ‚Üê max{U, QÃ≤ + Œ≥^{-t} œµ}
    # 10.   a‚Ä≤ ‚Üê argmax_a QÃÑ(b,a)
    # 11.   o‚Ä≤ ‚Üê argmax_o p(o|b,a‚Ä≤) (VÃÑ(œÑ(b,a‚Ä≤,o)) - VÃ≤(œÑ(b,a‚Ä≤,o)))
    # 12.   Calculate L_t so that L‚Ä≤ = ‚àë_s R(s,a‚Ä≤)b(s) + Œ≥ ( p(o‚Ä≤|b,a‚Ä≤)L_t + ‚àë_{o‚â†o‚Ä≤} p(o|b,a‚Ä≤)VÃ≤(œÑ(b,a‚Ä≤,o)) )
    # 13.   Calculate U_t so that U‚Ä≤ = ‚àë_s R(s,a‚Ä≤)b(s) + Œ≥ ( p(o‚Ä≤|b,a‚Ä≤)U_t + ‚àë_{o‚â†o‚Ä≤} p(o|b,a‚Ä≤)VÃ≤(œÑ(b,a‚Ä≤,o)) )
    # 14.   b‚Ä≤ ‚Üê œÑ(b,a‚Ä≤,o‚Ä≤)
    # 15.   Insert b‚Ä≤ into T_R as a child of b.
    # 16.   SamplePoints(T_r, Œì, b‚Ä≤, L_t, U_t, œµ, t+1)
end

# ========== needed functions ==========

function PRUNE(tree::BeliefTree, Œì::AlphaVectorPolicy) # not sure what parameters needed h;ere
    # see section III.D

    # insert code to determine which belief-action pair to prune?

    # BN = BeliefNode
    # a = action (not ActionNode)
    prune_tree(tree, BN, a)

    nothing
end



# ========== helper functions ==========
argmax_(f, domain) = domain[argmax(f, domain)]