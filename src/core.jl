# ========== Algorithm 1 ==========
function SARSOP_main(solver::SARSOPSolver, pomdp::POMDP)
    # 1. Initialize the set Œì of Œ±-vectors, representing the lower bound VÃ≤ on the optimal value function V‚àó. Initialize the upper bound VÃÑ on V‚àó.
    # 2. Insert the initial belief point b0 as the root of the tree T_R.
    # 3. repeat 
    # 4. SAMPLE(T_R, Œì)
    # 5.    Choose a subset of nodes from T_R. For each chosen node b, BACKUP(T_R,Œì,b).
    # 6.    PRUNE(T_R, Œì)
    # 7. until termination conditions are satisfied
    # 8. return Œì


    # how to initialize lower bound alpha vectors?
    # Œ±_vectors = ?????
    # action_map = ?????
    Œì = AlphaVectorPolicy(pomdp, Œ±_vectors, action_map)

    # how to initialize upper bound VÃÑ?
    # VÃÑ = 

    # how to represent belief tree?
    b0 = ones(length(states(pomdp)))/length(states(pomdp)) # uniform weighted, probably wrong
    T_R = Tree(b0)

    terminal_condition = false
    while !terminal_condition
        Sample(T_R, Œì)

        subset_T_R = [] # build this somehow
        for b in subset_T_R
            Backup(T_R, Œì, b)
        end

        # possibly: 
        # for ii in rand(1:T_R.n_nodes)
        #     b = selectNode(T_R, ii) # this function would need to be made, not difficult
        #     Backup(T_R, Œì, b)
        # end

        PRUNE(T_R, Œì)
    end

    return Œì # return alpha vectors and corresponding actions
end

# ========== Algorithm 2 ==========
# Perform Œ±-vector backup at a node b of T_R
function Backup(T_R::Tree, Œì::AlphaVectorPolicy, b)
    # 1. Œ±_{a,o} ‚Üê argmax_Œ± (Œ± ‚ãÖ œÑ(b,a,o)), ‚àÄ a‚ààùíú, o‚ààùí™
    # 2. Œ±_a(s) ‚Üê R(s,a) + Œ≥ ‚àë_{o,s‚Ä≤} T(s,a,s‚Ä≤)Z(s‚Ä≤,a,o)Œ±_{a,o}(s‚Ä≤), ‚àÄ a‚ààùíú, s‚ààùíÆ
    # 3. Œ±‚Ä≤ ‚Üê argmax(Œ±_a ‚ãÖ b, for a in ùíú)
    # 4. Insert Œ±‚Ä≤ into Œì.

    # CURRENT ISSUE:
    # This should only be looking at the children of B, rather than the entire pomdp.
    # This is not stated in the algorithm, but rather in Section III.B 
    # Will need to reasses how to compute. Rather than iterate over all actions, observations,
    # should instead iterate over children of b.

    pomdp, Œ±_vectors, action_map = Œì.pomdp, Œì.Œ±_vectors, Œì.action_map

    ùíú = ordered_actions(pomdp)
    ùíÆ = ordered_states(pomdp)
    Œ≥ = discount(pomdp)

    Œ± = zeros( length(ùíÆ), length(ùíú) )
    for (ai, a) in ùíú
        for (si, s) in ùíÆ
            temp_sum = 0.0
            for (s‚Ä≤, T) in weighted_iterator(transition(pomdp, s, a))
                for (o, Z) in weighted_iterator(observation(pomdp, a, s‚Ä≤))
                    b‚Ä≤ = œÑ(b,a,o) # this is bad, should be looking at children. What to do if no children??
                    temp_sum += T * Z * argmax_(Œ±_ -> Œ±_ ‚ãÖ b‚Ä≤, Œ±_vectors)
                end
            end
            Œ±[si, ai] = reward(pomdp,s,a) + Œ≥ * temp_sum
        end
    end

    idx = argmax( vec( b' * Œ± ) ) # vec vs transpose for speed?
    push!(Œ±_vectors, Œ±[:, idx])
    push!(action_map, ùíú[idx])

    return AlphaVectorPolicy(pomdp, Œ±_vectors, action_map)
end

# ========== Algorithm 3 ==========
# Sampling near R*
function Sample(T_R::Tree, Œì::AlphaVectorPolicy)
    # 1. Set L to the current lower bound on the value function at the root b_0 of T_R. Set U to L + œµ, where œµ is the current target gap size at b0.
    # 2. SAMPLEPOINTS(T_R, Œì, b_0, L, U, œµ, 1).

    b = T_R.b0.b
    œµ = 0 # ??????????????????? wtf is this value supposed to be, definitely not zero
    L = maximum(Œ± ‚ãÖ T_R.b0 for Œ± in Œì)
    U = L + œµ 
    
    SAMPLEPOINTS(T_R, Œì, b, L, U, œµ, 1)
end

function SamplePoints(T_R::Tree, Œì::AlphaVectorPolicy, b, L, U, œµ, t)
    # 3. Let VÃÇ be the predicted value of V*(b).
    # 4. if VÃÇ ‚â§ L and VÃÑ ‚â§ max{U, VÃ≤(b) + œµŒ≥^{-t}} then
    # 5.    return
    # 6. else
    # 7.    QÃ≤ ‚Üê max_a QÃ≤(b,a)
    # 8.    L‚Ä≤ ‚Üê max{L, QÃ≤}
    # 9.    U‚Ä≤ ‚Üê max{U, QÃ≤ + Œ≥^{-t} œµ}
    # 10.   a‚Ä≤ ‚Üê argmax_a QÃÑ(b,a)
    # 11.   o‚Ä≤ ‚Üê argmax_o p(o|b,a‚Ä≤) (VÃÑ(œÑ(b,a‚Ä≤,o)) - VÃ≤(œÑ(b,a‚Ä≤,o)))
    # 12.   Calculate L_t so that L‚Ä≤ = ‚àë_s R(s,a‚Ä≤)b(s) + Œ≥ ( p(o‚Ä≤|b,a‚Ä≤)L_t + ‚àë_{o‚â†o‚Ä≤} p(o|b,a‚Ä≤)VÃ≤(œÑ(b,a‚Ä≤,o)) )
    # 13.   Calculate U_t so that U‚Ä≤ = ‚àë_s R(s,a‚Ä≤)b(s) + Œ≥ ( p(o‚Ä≤|b,a‚Ä≤)U_t + ‚àë_{o‚â†o‚Ä≤} p(o|b,a‚Ä≤)VÃ≤(œÑ(b,a‚Ä≤,o)) )
    # 14.   b‚Ä≤ ‚Üê œÑ(b,a‚Ä≤,o‚Ä≤)
    # 15.   Insert b‚Ä≤ into T_R as a child of b.
    # 16.   SamplePoints(T_r, Œì, b‚Ä≤, L_t, U_t, œµ, t+1)
end

# ========== needed functions ==========

function PRUNE(T_R::Tree, Œì::AlphaVectorPolicy)
    # see section III.D
end

# belief update - section III.B
# implement using DiscreteUpater package, as in HW 6?
function œÑ(b,a,o)
    # b‚Ä≤(s‚Ä≤) = œÑ(b,a,o) = Œ∑ Z(s‚Ä≤,a,o) ‚àë_s T(s,a,s‚Ä≤)b(s)
    return b‚Ä≤
end



# ========== helper functions ==========
argmax_(f, domain) = domain[argmax(f, domain)]